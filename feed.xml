<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://huudatdo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://huudatdo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-11T12:47:55+00:00</updated><id>https://huudatdo.github.io/feed.xml</id><title type="html">blank</title><subtitle>Undergraduate Student @ VinUniversity</subtitle><entry><title type="html">Random thoughts on Cognitive Psychology &amp;amp; Artificial Intelligence</title><link href="https://huudatdo.github.io/blog/2025/cognitive_psy/" rel="alternate" type="text/html" title="Random thoughts on Cognitive Psychology &amp;amp; Artificial Intelligence"/><published>2025-03-10T16:40:16+00:00</published><updated>2025-03-10T16:40:16+00:00</updated><id>https://huudatdo.github.io/blog/2025/cognitive_psy</id><content type="html" xml:base="https://huudatdo.github.io/blog/2025/cognitive_psy/"><![CDATA[<p>As a junior student majoring in Computer Science, I enrolled in the PSYC2050-Cognitive Psychology course in light of the desire to know more about how humans magically perceive the world around themselves and possess the extremely extraordinary intelligence that dominates all other animals existing throughout the history of this planet.</p> <p>P/s: I put the purple color text to indicate the question/problem that I found really intuitively intriguing and that I’m still working on.</p> <h2 id="week-2--3-basic-processes-in-visual-perception--object-and-face-recognition">Week 2 &amp; 3: Basic Processes in Visual Perception &amp; Object and Face Recognition</h2> <p>As we learn about the basic vision system, comprising color, depth perception, etc. I found this system is much more amazing than just seeing the world. Under the light wave going into the eyes, there are many processes that happen concurrently at an outstanding reflex speed, and then they are built upon that information again to finalize and reconstruct a mental state of the world. The lecture made me revisit the concept of vision in computers, which completely lacks understanding of the 3D worlds, even when people train the system to do every possible task that humans can do: classification, depth prediction, semantic segmentation, and object detection. Nevertheless, the system right now is still far away from being used for robots because of the aforementioned limitation. <span style="color:purple">Thus, I started to question the hidden dynamics of vision maybe not possessing the understanding of color or depth, they are just the consequences of our human learning to interact with the world since we were born.</span> As a matter of fact, other research shows that newborn babies do not interpret color or depth, and those abilities are gradually developed over time. Therefore, I started to believe that do develop an outstanding machine system, we need to incentivize the machines to develop their special vision system that might converge into the same vision system as humans.</p> <h2 id="week-4-attention-and-performance">Week 4: Attention and Performance</h2> <p>the current miraculous AI development has been hugely inspired by theories from neuroscience and cognitive psychology, and mimicking attention was the most significant work that led to powerful ChatGPT. While the definitions of “Early Selection” and “Late Selection” converge into the theory of “Semantic content drives attention” without a doubt, perceptual organization makes me ponder a lot.</p> <p>In terms of perceptual organization, we learned about the biases of humans in constructing meaningful parts from different abstract pictures. <span style="color:purple">But the question “Where do the biases come from?” stuck in my head hardly. There isn’t a specific form of those biases exhibited around us every day, why do we have those biases?</span> Currently, I’m hypothesizing that those biases are the methods our brain uses, trying to compositionally generalize the peculiar images into smaller parts that we can make sense of. On the other hand, I believe symmetry is another form of bias that we unconsciously utilize every day but it did not appear in the content. I would definitely think about this behavior more.</p> <p>Then, the definition of “serial search” and “pop-out effect” was introduced and I found them to be very astonishing and relatable to my current research about compositional generalization. The current generative text-to-image AI models do not have this type of processing, causing them to fall shortly when encountering complex text prompts. After the lecture, I quickly implement the idea of “serial search” into generative AI. For example, given an input text prompt of “red car and yellow bicycle”, I train the model to generate images for “red car” and “yellow bicycle” and ultimately based on those generated images produce the faithful output. As expected, the models performed tremendously well and I submitted the work to a big international conference.</p> <h2 id="week-5--6-classical-conditioning--operant-conditioning">Week 5 &amp; 6: Classical conditioning &amp; Operant conditioning</h2> <p>The lecture covered operant conditioning, emphasizing Skinner’s framework of learning through reinforcement and punishment, which shapes behavior using key processes such as determining base rates, training, extinction, and spontaneous recovery. Critical concepts include positive and negative reinforcement to increase behavior, and punishment to suppress it, with context cues (discriminative stimuli) playing a significant role. Advanced techniques like shaping and chaining were highlighted for building complex behaviors, while reinforcement schedules and superstitious behaviors demonstrated how patterns of rewards and expectations influence learning. The session also touched on vicarious learning, rooted in Bandura’s social learning theory, where individuals learn by observing others, showcasing the importance of modeling and the social context in behavior acquisition.</p> <p>Operant conditioning and vicarious learning have direct parallels to Reinforcement Learning in AI, where agents learn optimal behaviors through trial-and-error interactions with an environment, guided by rewards and penalties. RL leverages the principles of positive and negative reinforcement to maximize cumulative rewards, similar to Skinner’s focus on shaping and chaining behaviors through feedback mechanisms. However, a critical limitation of RL is its reliance on reward formulations, which oversimplify the complexities of real-world environments. <span style="color:purple">And in the sense that every ability is boiled down to learning as I pointed out in week 2, I wonder what are the main components that incentivize humans to develop all forms of intelligence.</span> Why do humans, with a small biological difference compared to chimpanzees, successfully evolve even when all creatures were born with no self-awareness about the environment? <span style="color:purple">What motivates the development of the vision system?</span></p> <h2 id="week-7--8-short-term-memory--working-memory">Week 7 &amp; 8: Short-term memory &amp; Working Memory</h2> <p>Sensory memory and short-term memory were explained using the modal model of memory, which emphasizes their role in information processing. Sensory memory, including iconic (visual) and echoic (auditory) memory, is characterized by a large capacity and brief duration, holding unprocessed information just long enough for attention to transfer to short-term memory. Short-term memory, on the other hand, has a limited capacity—often cited as 7±2 items (Miller, 1956) or around four items in recent studies (Cowan, 2021). It temporarily maintains information through rehearsal, but distractions can easily disrupt this retention. Techniques like chunking improve its efficiency by organizing data into meaningful units.</p> <p>As I reflect on the current state of AI systems, I can’t help but notice how far they are from replicating human sensory and short-term memory. In humans, sensory memory synchronizes inputs from all our senses—visual, auditory, tactile, and even olfactory—holding an incredible amount of raw data briefly in a pre-attentive state. This seamless integration allows us to prioritize and process information efficiently, transferring only what’s important to short-term memory for further manipulation. In contrast, AI systems process sensory inputs in isolation, without a shared temporal or contextual framework to unify them. For instance, when AI analyzes an image and an audio clip, it doesn’t inherently combine these data streams into a coherent sensory experience like we do.</p> <p>Short-term memory, or working memory, is another area where AI lags behind. My own working memory lets me actively hold and manipulate information, whether I’m doing mental arithmetic or solving complex problems. It evolves dynamically, shaped by experience and attention, allowing me to refine ideas or strategies incrementally. AI, however, relies on a fixed context window, like the token limits in large language models. These windows feel static to me—they can store a limited sequence of inputs but don’t refine or adapt their stored information as I would when tackling a problem. If I want to rework an idea, I naturally iterate and build on it, whereas AI needs explicit refeeding of data to do the same. <span style="color:purple">This lack of dynamic, integrated memory systems makes me realize how much more there is to achieve before AI truly mirrors human cognition.</span></p> <h2 id="week-9-procedural-memory">Week 9: Procedural memory</h2> <p>When I think about procedural memory, it reminds me of how we build skills over time through repeated practice, like riding a bike or typing on a keyboard. Once learned, these skills become automatic, requiring little conscious effort. In AI, this concept parallels transfer learning, where a model trained on one task can adapt its knowledge to a related task. For example, a neural network trained to recognize objects in one dataset can use its learned features to perform well on a different dataset with minimal additional training. Just as procedural memory allows me to apply skills flexibly in new situations, transfer learning enables AI to leverage prior knowledge efficiently, reducing the need to start learning from scratch every time.</p> <h2 id="week-10-explicit-memory">Week 10: Explicit memory</h2> <p>The theory of memory decay suggests that information stored in our memory weakens over time if it is not actively recalled or used. I hypothesize that this natural process serves an essential function in human cognition: it clears space for new knowledge while retaining the most valuable and frequently accessed information. Our brain, faced with finite resources, must prioritize efficiency, and memory decay might act as a filter to ensure that only relevant and useful information persists. This aligns with the idea that repeated use and reinforcement—through recall, practice, or emotional significance—consolidates memories into long-term storage, effectively marking them as important. Meanwhile, less critical or outdated information fades away, preventing our cognitive systems from being overwhelmed by unnecessary details. This dynamic balance reflects an adaptive mechanism that helps us stay focused on present and future challenges without being bogged down by an excess of rarely useful memories. <span style="color:purple">So even when we name it long-term memory, does it possess elasticity that paves the path to the adaptability of humans? It’s not just about retention; it’s about evolution, enabling us to synthesize complex concepts, create innovative solutions, and draw meaningful connections between past experiences and future goals.</span></p> <p>This adaptability is strikingly analogous to the concept of a “gene.” Just as genes encode information that evolves through mutations and natural selection, long-term memory evolves through exposure, reinforcement, and the pruning of unused connections. Genes allow the human race to adapt linearly over generations, shaping our biology to meet environmental demands. Similarly, long-term memory allows individuals to adapt within their lifetime, shaping cognition and behavior to navigate an ever-changing world. Both systems are not perfect archives but are instead optimized for resilience and flexibility. They prioritize what is valuable for survival and progress, whether it’s traits for an organism or ideas for a mind. <span style="color:purple">Hence, is that another factor hinders AI capabilities is digital computation where every number, and parameter are stored with absolute value. And if we want to move into analog computation with elasticity, we need to revisit the question: what will incentivize the evolution as we want?</span></p> <h2 id="week-1112-reasoning-problem-solving-and-creativity--improving-problem-solving">Week 11&amp;12: Reasoning, problem-solving, and creativity &amp; Improving problem-solving</h2> <p>In week 11 and week 12, we went through strategies humans use to tackle complex challenges. Key methods include analogy, where solutions to past problems are applied to new ones; means-end analysis, which involves creating subgoals to bridge gaps between current and goal states; difference reduction, focusing on minimizing discrepancies between present and desired outcomes; and alternative problem representations to facilitate understanding. Techniques like incubation allow unproductive fixations to fade, enabling insights and novel connections through breaks in conscious problem-solving. Tasks like the Wason selection task and the mutilated checkerboard problem showcase how representation and reasoning strategies impact outcomes.</p> <p>All these methods reflect different forms of compositional reasoning or compositional generalization. They rely on combining and reapplying smaller, well-understood knowledge elements to form complex solutions. Whether through analogies, breaking problems into subgoals, or restructuring representations, these approaches demonstrate the human ability to extrapolate and generalize across diverse scenarios. This type of reasoning is central to problem-solving but remains a significant limitation in current AI systems, as they struggle to replicate the flexible and creative compositional abilities inherent in human cognition.</p> <h2 id="week-13--14-human-intelligence--artificial-intelligence">Week 13 &amp; 14: Human Intelligence &amp; Artificial intelligence</h2> <p>Throughout the 2 weeks as well as the whole course, I fervently believe that compositional generalization—the ability to create and understand complex knowledge by composing primitive knowledge—is essential in how humans solve problems, ranging from mathematics and physics to daily life tasks. Nevertheless, despite the astonishing performance in language and vision, current AI systems do not exhibit compositional generalization. Furthermore, pretraining and scaling AI systems are hitting a wall, necessitating a more efficient training regime that exploits the hidden structure of data and extrapolates easily to out-of-distribution data, such as compositionality. Therefore, I fervently believe this lack of compositional generalization is the key drawback hindering AI from significantly closing the colossal gap with human-level reasoning.</p>]]></content><author><name></name></author><category term="random"/><category term="AI"/><category term="cognitive"/><summary type="html"><![CDATA[What did I learn in Cognitive Psychology course :)]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://huudatdo.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://huudatdo.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://huudatdo.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024[[read-time]] min read We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://huudatdo.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://huudatdo.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://huudatdo.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>